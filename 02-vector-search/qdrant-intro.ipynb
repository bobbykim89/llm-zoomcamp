{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd5d1f7",
   "metadata": {},
   "source": [
    "# Vector Search with Qdrant\n",
    "\n",
    "### Vector Search\n",
    "\n",
    "Vector search is the backbone of the modern internet, whether you notice it or not. It powers recommendation engines, chatbots, AI agents, and even major search engines.\n",
    "\n",
    "In simple terms, traditional keyword search works by matching exact words. This works well when you know the precise keywords present in the data. But what happens when there are no keywords? What if you're searching through images, audio, video or code, or even cross-modally?\n",
    "\n",
    "Moreover, even in text-heavy documents, keyword search struggles to capture context and meaning. The same idea can be phrased in countless ways, so it is completely unfeasible to compare/search for using keyword-based methods.\n",
    "\n",
    "Instead of relying on exact matches, vector search retrieves information based on semantic similarity measured numerically between vectorized data representations (embeddings). It recognizes patterns and relationships between concepts, enabling search systems to retrieve the most relevant content, even when the phrasing differs, terminology varies, or no explicit keywords exist.\n",
    "\n",
    "### Qdrant\n",
    "\n",
    "[Qdrant](https://qdrant.tech/) is an **open-source** vector search engine, a dedicated solution built in Rust for scalable vector search. If you're wondering why you might need a dedicated solution for vector search, we’ve addressed that in the article [\"Built for Vector Search\"](https://qdrant.tech/articles/dedicated-vector-search/).\n",
    "\n",
    "To TLDR:\n",
    "- To make production-level vector search at scale;\n",
    "- To stay in sync with the latest trends and best practices;\n",
    "- To fully use vector search capabilities (including those beyond simple similarity search).\n",
    "\n",
    "In this notebook, we’ll give you a small sneak peek into semantic (vector) search with Qdrant and encourage you to play around & see if it fits your needs!\n",
    "\n",
    "If you have any questions about vector search in Qdrant, feel free to reach out in our [Discord community](https://discord.com/invite/G7PQU6Cy).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f256cee",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "Qdrant is fully open-source, which means you can run it in multiple ways depending on your needs.\n",
    "You can self-host it on your own infrastructure, deploy it on Kubernetes, or run it in managed Cloud.\n",
    "\n",
    "We're going to run a Qdrant instance in a Docker container.\n",
    "\n",
    "#### Docker\n",
    "\n",
    "All you need to do is pull the image and start the container using the following commands:\n",
    "\n",
    "```sh\n",
    "docker pull qdrant/qdrant\n",
    "\n",
    "docker run -p 6333:6333 -p 6334:6334 \\\n",
    "   -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
    "   qdrant/qdrant\n",
    "\n",
    "```\n",
    "\n",
    "The second line in the `docker run` command mounts local storage to keep your data persistent. So even if you restart or delete the container, your data will still be stored locally.\n",
    "- 6333 – REST API port\n",
    "- 6334 – gRPC API port\n",
    "\n",
    "To help you explore your data visually, Qdrant provides a built-in Web UI, available in both Qdrant Cloud and local instances. You can use it to inspect collections, check system health, and even run simple queries.\n",
    "\n",
    "When you're running Qdrant in Docker, the Web UI is available at http://localhost:6333/dashboard\n",
    "\n",
    "#### Installing Required Libraries\n",
    "\n",
    "In the environment you created specifically for this course, we’ll install:\n",
    "- The `qdrant-client` package. We'll be using the Python client, but Qdrant also offers official clients for JavaScript/- - TypeScript, Go, and Rust, so you can choose the best fit for your own projects.\n",
    "- The `fastembed` package - an optimized embedding (data vectorization) solution designed specifically for Qdrant. Make sure you install version >= 1.14.2 to use the local inference with Qdrant.\n",
    "\n",
    "```sh\n",
    "!python -m pip install -q \"qdrant-client[fastembed]>=1.14.2\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443d274",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries & Connect to Qdrant\n",
    "\n",
    "Now let’s import the necessary modules from the `qdrant-client` package.\n",
    "\n",
    "The `QdrantClient` class allows us to establish a connection to the Qdrant service,\n",
    "while the `models` module provides definitions for various configurations and parameters we’ll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3956f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "import requests\n",
    "from fastembed import TextEmbedding\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "692508cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to local Qdrant instance\n",
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b33e7b",
   "metadata": {},
   "source": [
    "## Step 2: Study the Dataset\n",
    "To build a working vector search solution (and, more generally, to understand if/when/how it’s needed), it's good to study the dataset and figure out the nature and structure of the data we’re working with, for example:\n",
    "- modality — is it text, images, videos, a combination?\n",
    "- specifics — if it’s text: language used, how big are the text pieces, are there any special characters, etc.\n",
    "\n",
    "It will help us define:\n",
    "- the right data \"schema\" (what to vectorize, what to store as metadata, etc);\n",
    "- the right embedding model (the best fit based on the domain, precision & resource requirements).\n",
    "\n",
    "We have a toy dataset provided for experimentation, let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b967a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faac4e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - When will the course start?'},\n",
       " {'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites',\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - What are the prerequisites for this course?'},\n",
       " {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - Can I still join the course after the start date?'},\n",
       " {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?'},\n",
       " {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - What can I do before the course starts?'},\n",
       " {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - how many Zoomcamps in a year?'},\n",
       " {'text': 'Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..',\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - Is the current cohort going to be different from the previous cohort?'},\n",
       " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - Can I follow the course after it finishes?'},\n",
       " {'text': 'Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.',\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - Can I get support if I take the course in the self-paced mode?'},\n",
       " {'text': 'All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\\nh\\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-',\n",
       "  'section': 'General course-related questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'question': 'Course - Which playlist on YouTube should I refer to?'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    for doc in course['documents']:\n",
    "        course = {\n",
    "            \"text\": doc['text'],\n",
    "            \"section\": doc['section'],\n",
    "            \"course\": course['course'],\n",
    "            \"question\": doc['question']\n",
    "        }\n",
    "        flattened_documents.append(course)\n",
    "\n",
    "flattened_documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de41277",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Data already seems cleaned and chunked (i.e., divided into small pieces that embedding models can easily digest), so what's left is to define:\n",
    "- which fields could be used for semantic search ;\n",
    "- which fields should be stored as metadata, e.g. useable for filtering conditions;\n",
    "\n",
    "We have a dataset with three course types:\n",
    "`data-engineering-zoomcamp`, `machine-learning-zoomcamp`, and `mlops-zoomcamp`.\n",
    "Each course includes a collection of `question` and `text` (answer) pairs, along with the `section` the question refers to.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e1418",
   "metadata": {},
   "source": [
    "#### Which Fields Could Be Used for Semantic Search\n",
    "\n",
    "Here we can observe semantic similarity in practice: some of the questions and answers don’t share many overlapping words,\n",
    "yet they clearly address the same topic. One asks about a topic, the other provides an answer.\n",
    "\n",
    "For example:\n",
    "\n",
    "Question:\n",
    "- “I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?”\n",
    "\n",
    "Answer:\n",
    "- “You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.”\n",
    "\n",
    "These two could be matched via the keyword registered,\n",
    "but a sentence like “Not registered participants are not getting certification” would also match that keyword, while having a different semantic meaning.\n",
    "\n",
    "So, if we’re building a Q&A retrieval-augmented generation (RAG) system,\n",
    "it makes sense to store the text field (answers) as embeddings, and use vector search to find the most relevant answer to a given `question` query.\n",
    "\n",
    "#### Which Fields Should Be Stored as Metadata\n",
    "\n",
    "For example, we could store the course and section fields as metadata.\n",
    "This way, we can filter search results when asking questions related to a specific course or a specific section.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64699d",
   "metadata": {},
   "source": [
    "## Step 3: Choosing the Embedding Model with FastEmbed\n",
    "\n",
    "Now that we know we're embedding small chunks of English text (course-related question and answer pairs), we can choose a suitable embedding model to convert this data into vectors.\n",
    "\n",
    "The choice of an embedding model depends on many factors:\n",
    "- The task, data modality, and data specifics;\n",
    "- The trade-off between search precision and resource usage (larger embeddings require more storage and memory);\n",
    "- The cost of inference (especially if you're using a third-party provider);\n",
    "etc\n",
    "\n",
    "> The best way to select an embedding model is to test and benchmark different options on your own data.\n",
    "\n",
    "In this notebook, we’re going to use [FastEmbed](https://github.com/qdrant/fastembed) as our embedding provider.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb86a0",
   "metadata": {},
   "source": [
    "**FastEmbed** is an optimized embedding solution designed specifically for Qdrant. It delivers low-latency, CPU-friendly embedding generation, eliminating the need for heavy frameworks like PyTorch or TensorFlow. It uses quantized model weights and ONNX Runtime, making it significantly faster than traditional Sentence Transformers on CPU while maintaining competitive accuracy.\n",
    "\n",
    "FastEmbed supports:\n",
    "- Dense embeddings for text and images (the most common type in vector search, ones we're going to use today)\n",
    "- Sparse embeddings (e.g., BM25 and sparse neural embeddings)\n",
    "- Multivector embeddings (e.g., ColPali and ColBERT, late interaction models)\n",
    "- Rerankers\n",
    "\n",
    "All of these can be directly used in Qdrant (as Qdrant supports dense, sparse & multivectors along with hybrid search).\n",
    "FastEmbed’s integration with Qdrant allows you to directly pass text or images to the Qdrant client for embedding.\n",
    "\n",
    "In this notebook, we’ll use FastEmbed for local inference with Qdrant.\n",
    "\n",
    "> Keep in mind your machine's resources when choosing an embedding model for local inference.\n",
    "\n",
    "FastEmbed for Textual Data\n",
    "Let’s select an embedding model to use for our course question answers, stored in text fields, from the options supported by FastEmbed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a1c5e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'BAAI/bge-base-en',\n",
       "  'sources': {'hf': 'Qdrant/fast-bge-base-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.42,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-base-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-base-en-v1.5-onnx-q',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.21,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-large-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-large-en-v1.5-onnx',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 1.2,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en',\n",
       "  'sources': {'hf': 'Qdrant/bge-small-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-small-en-v1.5-onnx-q',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.067,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-zh-v1.5',\n",
       "  'sources': {'hf': 'Qdrant/bge-small-zh-v1.5',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'mixedbread-ai/mxbai-embed-large-v1',\n",
       "  'sources': {'hf': 'mixedbread-ai/mxbai-embed-large-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-xs',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-xs',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-s',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-s',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-m',\n",
       "  'sources': {'hf': 'Snowflake/snowflake-arctic-embed-m',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.43,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-m-long',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-m-long',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 2048 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.54,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-l',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-l',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 1.02,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-clip-v1',\n",
       "  'sources': {'hf': 'jinaai/jina-clip-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/text_model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text&image), English, Prefixes for queries/documents: not necessary, 2024 year',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.55,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'Qdrant/clip-ViT-B-32-text',\n",
       "  'sources': {'hf': 'Qdrant/clip-ViT-B-32-text',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.25,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'sources': {'hf': 'qdrant/all-MiniLM-L6-v2-onnx',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 256 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-en',\n",
       "  'sources': {'hf': 'xenova/jina-embeddings-v2-base-en',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-small-en',\n",
       "  'sources': {'hf': 'xenova/jina-embeddings-v2-small-en',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.12,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-de',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-de',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model_fp16.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (German, English), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.32,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-code',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-code',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (English, 30 programming languages), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-zh',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-zh',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), supports mixed Chinese-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-es',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-es',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), supports mixed Spanish-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'thenlper/gte-base',\n",
       "  'sources': {'hf': 'thenlper/gte-base',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'General text embeddings, Unimodal (text), supports English only input text, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.44,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'thenlper/gte-large',\n",
       "  'sources': {'hf': 'qdrant/gte-large-onnx',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 1.2,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1.5-Q',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model_quantized.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       "  'sources': {'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2019 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.22,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
       "  'sources': {'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 384 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 1.0,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'intfloat/multilingual-e5-large',\n",
       "  'sources': {'hf': 'qdrant/multilingual-e5-large-onnx',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~100 languages), 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 2.24,\n",
       "  'additional_files': ['model.onnx_data'],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v3',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v3',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Multi-task unimodal (text) embedding model, multi-lingual (~100), 1024 tokens truncation, and 8192 sequence length. Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'cc-by-nc-4.0',\n",
       "  'size_in_GB': 2.29,\n",
       "  'additional_files': ['onnx/model.onnx_data'],\n",
       "  'dim': 1024,\n",
       "  'tasks': {'retrieval.query': 0,\n",
       "   'retrieval.passage': 1,\n",
       "   'separation': 2,\n",
       "   'classification': 3,\n",
       "   'text-matching': 4}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097071b",
   "metadata": {},
   "source": [
    "It makes sense to choose a model that produces small-to-moderate-sized embeddings (e.g., 512 dimensions), so we don’t overuse resources in our simple setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e5e0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"BAAI/bge-small-zh-v1.5\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"Qdrant/bge-small-zh-v1.5\",\n",
      "    \"url\": \"https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz\",\n",
      "    \"_deprecated_tar_struct\": true\n",
      "  },\n",
      "  \"model_file\": \"model_optimized.onnx\",\n",
      "  \"description\": \"Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.\",\n",
      "  \"license\": \"mit\",\n",
      "  \"size_in_GB\": 0.09,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n",
      "{\n",
      "  \"model\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "    \"url\": null,\n",
      "    \"_deprecated_tar_struct\": false\n",
      "  },\n",
      "  \"model_file\": \"model.onnx\",\n",
      "  \"description\": \"Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year\",\n",
      "  \"license\": \"mit\",\n",
      "  \"size_in_GB\": 0.25,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n",
      "{\n",
      "  \"model\": \"jinaai/jina-embeddings-v2-small-en\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"xenova/jina-embeddings-v2-small-en\",\n",
      "    \"url\": null,\n",
      "    \"_deprecated_tar_struct\": false\n",
      "  },\n",
      "  \"model_file\": \"onnx/model.onnx\",\n",
      "  \"description\": \"Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.\",\n",
      "  \"license\": \"apache-2.0\",\n",
      "  \"size_in_GB\": 0.12,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "\n",
    "for model in TextEmbedding.list_supported_models():\n",
    "    if model[\"dim\"] == EMBEDDING_DIMENSIONALITY:\n",
    "        print(json.dumps(model, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422abafc",
   "metadata": {},
   "source": [
    "We need an embedding model suitable for English text.\n",
    "\n",
    "It also makes sense to select a unimodal model, since we’re not including images in our search, and specifically tailored solutions are usually better than universal ones.\n",
    "\n",
    "It seems like `jina-embedding-small-en` is a good choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94347b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ad51b",
   "metadata": {},
   "source": [
    "Like most dense embedding models, `jina-embedding-small-en` was trained to measure semantic closeness using cosine similarity.\n",
    "You can find this information, for example, on the model’s [Hugging Face card](https://huggingface.co/jinaai/jina-embeddings-v2-small-en).\n",
    "\n",
    "> The parameters of the chosen embedding model, including the output embedding dimensions and the semantic similarity (distance) metric, are required to configure semantic search in Qdrant.\n",
    "\n",
    "Now we’re ready to configure and use Qdrant for semantic search. To fully understand what’s happening, here’s a quick overview of Qdrant’s core terminology:\n",
    "- `Points` are the central entity Qdrant works with.\\\n",
    "A point is a record consisting of an ID, a vector, and an optional payload.\n",
    "- A `collection` is a named set of points (i.e., vectors with optional payloads) that you can search within.\n",
    "Think of it as the container for your vector search solution, a single business problem solved.\n",
    "\n",
    "> Qdrant supports different types of vectors to enable different modes of data exploration and search (dense, sparse, multivectors, and named vectors).\n",
    "\n",
    "In this example, we’ll use the most common type, `dense vectors`.\n",
    "\n",
    "Embeddings capture the semantic essence of the data, while the payload holds structured metadata.\n",
    "This metadata becomes especially useful when applying filters or sorting during search. `Qdrant's payloads` can hold structured data like booleans, keywords, geo-locations, arrays, and nested objects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abcf0f6",
   "metadata": {},
   "source": [
    "## Step 4: Create a Collection\n",
    "When creating a [collection](https://qdrant.tech/documentation/concepts/collections/), we need to specify:\n",
    "\n",
    "- Name: A unique identifier for the collection.\n",
    "- Vector Configuration:\n",
    "    - Size: The dimensionality of the vectors.\n",
    "    - Distance Metric: The method used to measure similarity between vectors.\n",
    "\n",
    "There are additional parameters you can explore in our [documentation](https://qdrant.tech/documentation/concepts/collections/#create-a-collection). Moreover, you can configure other vector types in Qdrant beyond typical dense embeddings (f.e., for hybrid search). However, for this example, the simplest default configuration is sufficient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5ddc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the collection name\n",
    "collection_name = 'zoomcamp-rag'\n",
    "\n",
    "# Create the collection with specified vector parameters\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY, # Dimensionality of the vectors\n",
    "        distance=models.Distance.COSINE # Distance metric for similarity search\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875973f",
   "metadata": {},
   "source": [
    "## Step 5: Create, Embed & Insert Points into the Collection\n",
    "[Points](https://qdrant.tech/documentation/concepts/points/#points) are the core data entities in Qdrant. Each point consists of:\n",
    "\n",
    "1. `ID`. A unique identifier. Qdrant supports both 64-bit unsigned integers and UUIDs.\n",
    "2. `Vector`. The embedding that represents the data point in vector space.\n",
    "3. `Payload` (optional). Additional metadata as key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31acab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "id = 0\n",
    "\n",
    "for course in documents_raw:\n",
    "    for doc in course['documents']:\n",
    "        point = models.PointStruct(\n",
    "            id=id,\n",
    "            # Embed text locally with \"jinaai/jina-embeddings-v2-small-en\" from FastEmbed\n",
    "            vector=models.Document(text=doc['text'], model=model_handle),\n",
    "            payload={\n",
    "                \"text\": doc['text'],\n",
    "                \"section\": doc['section'],\n",
    "                \"course\": course['course']\n",
    "            } # save all needed metadata fields\n",
    "        )\n",
    "        points.append(point)\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4be890",
   "metadata": {},
   "source": [
    "Now we’re going to embed and upload points to our collection.\n",
    "\n",
    "First, FastEmbed will fetch&download the selected model `(path defaults to os.path.join(tempfile.gettempdir(), \"fastembed_cache\"))`, and perform inference directly on your machine.\n",
    "Then, the generated points will be upserted into the collection, and the vector index will be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a57e6bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert data to qdrant collection\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b1297",
   "metadata": {},
   "source": [
    "The speed of upsert mainly depends on the time spent on local inference.\n",
    "To speed this up, you could run FastEmbed on GPUs or use a machine with more resources.\n",
    "\n",
    "In addition to basic `upsert`, Qdrant supports batch upsert in both column- and record-oriented formats.\n",
    "\n",
    "The Python client offers:\n",
    "- Parallelization\n",
    "- Retries\n",
    "- Lazy batching\n",
    "\n",
    "These can be configured via parameters in the `upload_collection` and `upload_points` functions.\n",
    "For details, check the [documentation](https://qdrant.tech/documentation/concepts/points/#upload-points)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415df2d",
   "metadata": {},
   "source": [
    "### Study Data Visually\n",
    "\n",
    "Let’s explore the uploaded data in the Qdrant Web UI at http://localhost:6333/dashboard to study semantic similarity visually.\n",
    "\n",
    "For example, using the `Visualize` tab in the `zoomcamp-rag` collection, we can view all answers to the course questions (948 points) and see how they group together by meaning, additionally coloured by the course type.\n",
    "\n",
    "To do that, run the following command:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"limit\": 948,\n",
    "  \"color_by\": {\n",
    "    \"payload\": \"course\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "This 2D representation is the result of dimensionality reduction applied to `jina-embeddings`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e35622",
   "metadata": {},
   "source": [
    "## Step 6: Running a Similarity Search\n",
    "\n",
    "Now, let’s find the most similar `text` vector in Qdrant to a given query embedding - the most relevant answer to a given question.\n",
    "\n",
    "### How Similarity Search Works\n",
    "\n",
    "1. Qdrant compares the query vector to stored vectors (based on a vector index) using the distance metric defined when creating the collection.\n",
    "2. The closest matches are returned, ranked by similarity.\n",
    "    > Vector index is built for approximate nearest neighbor (ANN) search, making large-scale vector search feasible.\n",
    "\n",
    "If you'd like to dive into our choice of vector index for vector search, check our article [\"What is a vector database\"](https://qdrant.tech/articles/what-is-a-vector-database/), or, for a more technical deep dive, our article on [Filterable Hierarchical Navigable Small World](https://qdrant.tech/articles/filtrable-hnsw/).\n",
    "\n",
    "Let's define a search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34a44376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search (query, limit=1):\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document( # embed the query text locally with \"jinaai/jina-embeddings-v2-small-en\"\n",
    "            text=query,\n",
    "            model=model_handle\n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True # to get metadata in the results\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75253c8",
   "metadata": {},
   "source": [
    "Now let’s pick a random question from the course data.\\\n",
    "As you remember, we didn’t upload the questions to Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33e7b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .\",\n",
      "  \"section\": \"Module 5: pyspark\",\n",
      "  \"question\": \"Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "course = random.choice(documents_raw)\n",
    "course_piece = random.choice(course['documents'])\n",
    "print(json.dumps(course_piece, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f54540",
   "metadata": {},
   "source": [
    "Let's see which answer we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aafd8c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(points=[ScoredPoint(id=355, version=0, score=0.8414844, payload={'text': \"Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\\nIf this does not work try to change other versions found in this repository.\\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)\", 'section': 'Module 5: pyspark', 'course': 'data-engineering-zoomcamp'}, vector=None, shard_key=None, order_value=None)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = search(course_piece['question'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21912c99",
   "metadata": {},
   "source": [
    "`score` – the cosine similarity between the `question` and text `embeddings`.\\\n",
    "Let’s compare the original and retrieved answers for our randomly selected question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e108fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)\n",
      "\n",
      "Top Retrieved Answer:\n",
      "Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\n",
      "If this does not work try to change other versions found in this repository.\n",
      "For more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)\n",
      "\n",
      "Original Answer:\n",
      "You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question:\\n{course_piece['question']}\\n\")\n",
    "print(\"Top Retrieved Answer:\\n{}\\n\".format(result.points[0].payload['text']))\n",
    "print(\"Original Answer:\\n{}\".format(course_piece['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cbd22",
   "metadata": {},
   "source": [
    "Now let’s search the answer to a question that wasn’t in the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d254d1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\n",
      "Older news:[source1] [source2]\n"
     ]
    }
   ],
   "source": [
    "print(search(\"What if I submit homeworks late?\").points[0].payload['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059484a",
   "metadata": {},
   "source": [
    "## Step 7: Running a Similarity Search with Filters\n",
    "\n",
    "We can refine our search using metadata filters.\n",
    "\n",
    "> Qdrant’s custom vector index implementation, Filterable HNSW, allows for precise and scalable vector search with filtering conditions.\n",
    "\n",
    "For example, we can search for an answer to a question related to a specific course from the three available in the dataset.\n",
    "Using a `must` filter ensures that all specified conditions are met for a data point to be included in the search results.\n",
    "\n",
    "> Qdrant also supports other filter types such as `should`, `must_not`, `range`, and more. For a full overview, check our [Filtering Guide](https://qdrant.tech/articles/vector-search-filtering/)\n",
    "\n",
    "To enable efficient filtering, we need to turn on [indexing of payload fields](https://qdrant.tech/documentation/concepts/indexing/#payload-index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b66004a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=2, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name='course',\n",
    "    field_schema='keyword' # exact matching on string metadata fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3affb91",
   "metadata": {},
   "source": [
    "Now let's update our search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e3ee7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_course (query: str, course='mlops-zoomcamp', limit=1):\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(  # embed the query text locally with \"jinaai/jina-embeddings-v2-small-en\"\n",
    "            text=query,\n",
    "            model=model_handle\n",
    "        ),\n",
    "        query_filter=models.Filter( # filter by course name\n",
    "            must= [\n",
    "                models.FieldCondition(\n",
    "                    key='course',\n",
    "                    match=models.MatchValue(value=course)\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True # to get metadata in the results\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02910906",
   "metadata": {},
   "source": [
    "Let’s see how the same question is answered across different courses:\\\n",
    "`data-engineering-zoomcamp`, `machine-learning-zoomcamp`, and `mlops-zoomcamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33e32d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-engineering-zoomcamp: No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\n",
      "Older news:[source1] [source2]\n",
      "machine-learning-zoomcamp: Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.\n",
      "(Added by Rileen Sinha, based on answer by Alexey on Slack)\n",
      "mlops-zoomcamp: Please choose the closest one to your answer. Also do not post your answer in the course slack channel.\n"
     ]
    }
   ],
   "source": [
    "search_question = \"What if I submit the homeworks late?\"\n",
    "courses_list = [\"data-engineering-zoomcamp\", \"machine-learning-zoomcamp\", \"mlops-zoomcamp\"]\n",
    "\n",
    "for course in courses_list:\n",
    "    search_result = search_in_course(query=search_question, course=course).points[0].payload['text']\n",
    "    print(f\"{course}: {search_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392bfe12",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "🎉 Congratulations! You now have everything you need to run a simple semantic search with Qdrant! 👏\n",
    "\n",
    "In general, data preparation, organization, and storage in a production-ready vector search solution is a topic worth a course of its own.\n",
    "If you’re curious to dive deeper into efficient vector search setup, check out our [Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/).\n",
    "\n",
    "In the next videos, we will show you how to use [hybrid search](https://qdrant.tech/articles/hybrid-search/), combining the strengths of both keywords-based search and vector search. In many real-world applications, they work hand-in-hand, balancing the precision of keywords with the flexibility of embeddings to deliver the best results.\n",
    "\n",
    "P.S. We encourage you to check out Qdrant’s capabilities, which go beyond similarity search powering RAG & agentic pipelines (but still, here's our [MCP server](https://github.com/qdrant/mcp-server-qdrant))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b99b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
